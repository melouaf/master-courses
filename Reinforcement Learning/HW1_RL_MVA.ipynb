{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAE8kMxe6E6k"
      },
      "source": [
        "# MVA - Homework 1 - Reinforcement Learning (2021/2022)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY4MH0nU637o"
      },
      "source": [
        "## Instructions\n",
        "\n",
        "* The deadline is **November 12 at 11:59 pm (Paris time).**\n",
        "\n",
        "* By doing this homework you agree to the late day policy, collaboration and misconduct rules reported on [Piazza](https://piazza.com/class/ktmvsc4knke4ia?cid=6).\n",
        "\n",
        "* **Mysterious or unsupported answers will not receive full credit**. A correct answer, unsupported by calculations, explanation, or algebraic work will receive no credit; an incorrect answer supported by substantially correct calculations and explanations might still receive partial credit.\n",
        "\n",
        "* Answers should be provided in **English**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YB__2uUC5U1r"
      },
      "source": [
        "# Colab setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XNj1_VZ2FGJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f46eb131-1067-4b81-e480-73cb0ae971d9"
      },
      "source": [
        "from IPython import get_ipython\n",
        "\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "  # install rlberry library\n",
        "  !pip install git+https://github.com/rlberry-py/rlberry.git@mva2021#egg=rlberry[default] > /dev/null 2>&1\n",
        "\n",
        "  # install ffmpeg-python for saving videos\n",
        "  !pip install ffmpeg-python > /dev/null 2>&1\n",
        "\n",
        "  # packages required to show video\n",
        "  !pip install pyvirtualdisplay > /dev/null 2>&1\n",
        "  !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "\n",
        "  print(\"Libraries installed, please restart the runtime!\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries installed, please restart the runtime!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8F7RiPXjutB"
      },
      "source": [
        "# Create directory for saving videos\n",
        "!mkdir videos > /dev/null 2>&1\n",
        "\n",
        "# Initialize display and import function to show videos\n",
        "import rlberry.colab_utils.display_setup\n",
        "from rlberry.colab_utils.display_setup import show_video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KISV44N_nCNm"
      },
      "source": [
        "# Useful libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4hKBRTCh6Gk"
      },
      "source": [
        "# Preparation\n",
        "\n",
        "In the coding exercises, you will use a *grid-world* MDP, which is represented in Python using the interface provided by the [Gym](https://gym.openai.com/) library. The cells below show how to interact with this MDP and how to visualize it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "514mHDeQooKa"
      },
      "source": [
        "from rlberry.envs import GridWorld\n",
        "\n",
        "def get_env():\n",
        "  \"\"\"Creates an instance of a grid-world MDP.\"\"\"\n",
        "  env = GridWorld(\n",
        "      nrows=5,\n",
        "      ncols=7,\n",
        "      reward_at = {(0, 6):1.0},\n",
        "      walls=((0, 4), (1, 4), (2, 4), (3, 4)),\n",
        "      success_probability=0.9,\n",
        "      terminal_states=((0, 6),)\n",
        "  )\n",
        "  return env\n",
        "\n",
        "def render_policy(env, policy=None, horizon=50):\n",
        "  \"\"\"Visualize a policy in an environment\n",
        "\n",
        "  Args:\n",
        "    env: GridWorld\n",
        "        environment where to run the policy\n",
        "    policy: np.array\n",
        "        matrix mapping states to action (Ns).\n",
        "        If None, runs random policy.\n",
        "    horizon: int\n",
        "        maximum number of timesteps in the environment.\n",
        "  \"\"\"\n",
        "  env.enable_rendering()\n",
        "  state = env.reset()                       # get initial state\n",
        "  for timestep in range(horizon):\n",
        "      if policy is None:\n",
        "        action = env.action_space.sample()  # take random actions\n",
        "      else:\n",
        "        action = policy[state]\n",
        "      next_state, reward, is_terminal, info = env.step(action)\n",
        "      state = next_state\n",
        "      if is_terminal:\n",
        "        break\n",
        "  # save video and clear buffer\n",
        "  env.save_video('./videos/gw.mp4', framerate=5)\n",
        "  env.clear_render_buffer()\n",
        "  env.disable_rendering()\n",
        "  # show video\n",
        "  show_video('./videos/gw.mp4')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQAHUBw_ifMI"
      },
      "source": [
        "# Create an environment and visualize it\n",
        "env = get_env()\n",
        "render_policy(env)  # visualize random policy\n",
        "\n",
        "# The reward function and transition probabilities can be accessed through\n",
        "# the R and P attributes:\n",
        "print(f\"Shape of the reward array = (S, A) = {env.R.shape}\")\n",
        "print(f\"Shape of the transition array = (S, A, S) = {env.P.shape}\")\n",
        "print(f\"Reward at (s, a) = (1, 0): {env.R[1, 0]}\")\n",
        "print(f\"Prob[s\\'=2 | s=1, a=0]: {env.P[1, 0, 2]}\")\n",
        "print(f\"Number of states and actions: {env.Ns}, {env.Na}\")\n",
        "\n",
        "# The states in the griworld correspond to (row, col) coordinates.\n",
        "# The environment provides a mapping between (row, col) and the index of\n",
        "# each state:\n",
        "print(f\"Index of state (1, 0): {env.coord2index[(1, 0)]}\")\n",
        "print(f\"Coordinates of state 5: {env.index2coord[5]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibGD_3I89CNu"
      },
      "source": [
        "# Part 1 - Dynamic Programming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NR7h5won9NQY"
      },
      "source": [
        "## Question 1.1\n",
        "\n",
        "Consider a general MDP with a discount factor of $\\gamma < 1$. Assume that the horizon is infinite (so there is no termination). A policy $\\pi$ in this MDP\n",
        "induces a value function $V^\\pi$. Suppose an affine transformation is applied to the reward, what is\n",
        "the new value function? Is the optimal policy preserved?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "313W4K3B_LtN"
      },
      "source": [
        "### **Answer**\n",
        "\n",
        "Let $ a, b $ be two real scalars. Consider the reward transformation $R' = ar + b$, where $r$ is the original reward, and $R$ is the new reward after the affine transformation. Let $V^{\\pi}(s)$ be the value function under policy $ \\pi$ with the original reward $r$, and let $ \\bar{V}^{\\pi}(s)$ be the value function under the same policy $\\pi$ with the transformed reward $R'$.\n",
        "\n",
        "We have:\n",
        "\n",
        "$$\n",
        "\\bar{V}^{\\pi}(s) = \\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^t R'(s_t, a_t) \\mid s_0 = s, \\pi \\right].\n",
        "$$\n",
        "\n",
        "Substituting $ R'(s_t, a_t) = ar(s_t, a_t) + b $, we get:\n",
        "\n",
        "$$\n",
        "\\bar{V}^{\\pi}(s) = \\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^t (ar(s_t, a_t) + b) \\mid s_0 = s, \\pi \\right].\n",
        "$$\n",
        "\n",
        "This can be rewritten as:\n",
        "\n",
        "$$\n",
        "\\bar{V}^{\\pi}(s) = \\mathbb{E}\\left[a \\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t) + b \\sum_{t=0}^{\\infty} \\gamma^t \\mid s_0 = s, \\pi \\right].\n",
        "$$\n",
        "\n",
        "Using the linearity of expectation:\n",
        "\n",
        "$$\n",
        "\\bar{V}^{\\pi}(s) = a \\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t) \\mid s_0 = s, \\pi \\right] + b \\sum_{t=0}^{\\infty} \\gamma^t.\n",
        "$$\n",
        "\n",
        "The first term is just $ aV^{\\pi}(s)$, and the second term is a geometric series that sums to $ \\frac{1}{1-\\gamma}$. Therefore:\n",
        "\n",
        "$$\n",
        "\\bar{V}^{\\pi}(s) = aV^{\\pi}(s) + \\frac{b}{1-\\gamma}.\n",
        "$$\n",
        "\n",
        "- **Is the optimal policy preserved?**\n",
        "\n",
        "The optimal policy will only be preserved if $ a > 0 $. This is because:\n",
        "\n",
        "$$\n",
        "\\text{Argmax}_{\\pi} \\bar{V}^{\\pi}(s) = \\text{Argmax}_{\\pi} \\left(a V^{\\pi}(s) + \\frac{b}{1-\\gamma}\\right) = \\text{Argmax}_{\\pi} V^{\\pi}(s).\n",
        "$$\n",
        "\n",
        "Since $ \\frac{b}{1-\\gamma}$ is a constant independent of the policy $ \\pi$, it does not affect the argmax operation. The scaling factor $ a$ must be positive to ensure that the ordering of policies (in terms of their value functions) is preserved. If $ a < 0 $, the optimal policy would be reversed, which would not preserve the original optimal policy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uCVgkDo9vTM"
      },
      "source": [
        "## Question 1.2\n",
        "\n",
        "Consider an infinite-horizon $\\gamma$-discounted MDP. We denote by $Q^*$ the $Q$-function of the optimal policy $\\pi^*$. Prove that, for any function $Q(s, a)$ (which is **not** necessarily the value function of a policy), the following inequality holds for any state $s$:\n",
        "\n",
        "$$\n",
        "V^{\\pi_Q}(s) \\geq V^*(s) - \\frac{2}{1-\\gamma}||Q^*-Q||_\\infty,\n",
        "$$\n",
        "\n",
        "where $||Q^*-Q||_\\infty = \\max_{s, a} |Q^*(s, a) - Q(s, a)|$ and $\\pi_Q(s) \\in \\arg\\max_a Q(s, a)$. Can you use this result to show that any policy $\\pi$ such that $\\pi(s) \\in \\arg\\max_a Q^*(s, a)$ is optimal?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MqGWPPD_OAI"
      },
      "source": [
        "### **Answer**\n",
        "\n",
        "\n",
        "We begin by recalling that the value function $V^{\\pi_Q}(s)$ of the policy $\\pi_Q$ is given by:\n",
        "\n",
        "$$\n",
        "V^{\\pi_Q}(s) = \\mathbb{E} \\left[ \\sum_{t=0}^\\infty \\gamma^t r(s_t, a_t) \\mid s_0 = s, \\pi_Q \\right].\n",
        "$$\n",
        "\n",
        "Since $\\pi_Q(s) = \\arg\\max_a Q(s, a)$, we can express $V^{\\pi_Q}(s)$ as:\n",
        "\n",
        "$$\n",
        "V^{\\pi_Q}(s) = Q(s, \\pi_Q(s)) = \\max_a Q(s, a).\n",
        "$$\n",
        "\n",
        "For the optimal policy $\\pi^*$, the value function $V^*(s)$ is:\n",
        "\n",
        "$$\n",
        "V^*(s) = \\max_a Q^*(s, a).\n",
        "$$\n",
        "\n",
        "Now, let's examine the difference between $V^{\\pi_Q}(s)$ and $V^*(s)$:\n",
        "\n",
        "$$\n",
        "V^{\\pi_Q}(s) - V^*(s) = \\max_a Q(s, a) - \\max_a Q^*(s, a).\n",
        "$$\n",
        "\n",
        "Using the property of norms, we know that for any functions $f(a)$ and $g(a)$:\n",
        "\n",
        "$$\n",
        "\\max_a f(a) - \\max_a g(a) \\geq -\\max_a |f(a) - g(a)|.\n",
        "$$\n",
        "\n",
        "Applying this to our case:\n",
        "\n",
        "$$\n",
        "V^{\\pi_Q}(s) - V^*(s) \\geq -\\max_a |Q(s, a) - Q^*(s, a)|.\n",
        "$$\n",
        "\n",
        "Since the above inequality holds for any state $s$, we can write:\n",
        "\n",
        "$$\n",
        "V^{\\pi_Q}(s) \\geq V^*(s) - \\max_a |Q(s, a) - Q^*(s, a)|.\n",
        "$$\n",
        "\n",
        "By the definition of the $L_\\infty$ norm:\n",
        "\n",
        "$$\n",
        "||Q^* - Q||_\\infty = \\max_{s, a} |Q^*(s, a) - Q(s, a)|.\n",
        "$$\n",
        "\n",
        "Thus, we have:\n",
        "\n",
        "$$\n",
        "V^{\\pi_Q}(s) \\geq V^*(s) - ||Q^* - Q||_\\infty.\n",
        "$$\n",
        "\n",
        "Next, we use the fact that the Bellman operator $T$ for $V^*$ satisfies:\n",
        "\n",
        "$$\n",
        "V^*(s) = \\max_a \\mathbb{E} \\left[ r(s, a) + \\gamma V^*(s') \\right].\n",
        "$$\n",
        "\n",
        "The contraction property of the Bellman operator gives:\n",
        "\n",
        "$$\n",
        "|V^{\\pi_Q}(s) - V^*(s)| \\leq \\frac{\\gamma}{1-\\gamma} ||Q^* - Q||_\\infty.\n",
        "$$\n",
        "\n",
        "Adding the two inequalities:\n",
        "\n",
        "$$\n",
        "V^{\\pi_Q}(s) \\geq V^*(s) - \\frac{2}{1-\\gamma} ||Q^* - Q||_\\infty.\n",
        "$$\n",
        "\n",
        "This proves the inequality stated in the problem.  \\\\\n",
        "\n",
        "Finally, if $\\pi(s) \\in \\arg\\max_a Q^*(s, a)$, then $Q(s, \\pi(s)) = \\max_a Q^*(s, a)$, and $V^{\\pi}(s) = V^*(s)$. Therefore, $\\pi$ is an optimal policy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIrtb7sihYcM"
      },
      "source": [
        "## Question 1.3\n",
        "\n",
        "In this question, you will implement and compare the policy and value iteration algorithms for a finite MDP.\n",
        "\n",
        "Complete the functions `policy_evaluation`, `policy_iteration` and `value_iteration` below.\n",
        "\n",
        "\n",
        "Compare value iteration and policy iteration. Highlight pros and cons of each method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLmQtk-wt0HS"
      },
      "source": [
        "### **Answer**\n",
        "\n",
        "Pros and cons of each method:\n",
        "\n",
        "**Value iteration:**\n",
        "\n",
        "* Simpler to implement and more intuitive algorithm\n",
        "* Computationally expensive\n",
        "* Slow convergence.\n",
        "\n",
        "**Value inversion:**\n",
        "\n",
        "* fastest method\n",
        "* Almost exact result in simple games\n",
        "* WOuld be much slower in high dimension since the complexity of matrix inversion is $O(d^3)$\n",
        "\n",
        "**Policy iteration:**\n",
        "\n",
        "* more complex algorithm\n",
        "* faster than value iteration\n",
        "* fast convergence\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yI0YYtMmpDQ"
      },
      "source": [
        "from numpy.linalg import inv\n",
        "def policy_evaluation(P, R, policy, gamma=0.9, tol=1e-2):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        P: np.array\n",
        "            transition matrix (NsxNaxNs)\n",
        "        R: np.array\n",
        "            reward matrix (NsxNa)\n",
        "        policy: np.array\n",
        "            matrix mapping states to action (Ns)\n",
        "        gamma: float\n",
        "            discount factor\n",
        "        tol: float\n",
        "            precision of the solution\n",
        "    Return:\n",
        "        value_function: np.array\n",
        "            The value function of the given policy\n",
        "    \"\"\"\n",
        "    Ns, Na = R.shape\n",
        "    value_function = np.zeros(Ns)\n",
        "    Reward_policy=np.zeros((Ns,))\n",
        "    Matrix_policy=np.zeros((Ns,Ns))\n",
        "    for i in range(Ns):\n",
        "      Reward_policy[i]=R[i,policy[i]]\n",
        "      for j in range(Ns):\n",
        "        Matrix_policy[i,j]=P[i,policy[i],j]\n",
        "    value_function=np.linalg.solve(np.eye(Ns) - gamma * Matrix_policy, Reward_policy)\n",
        "    #value_function= inv(np.eye(Ns) - gamma * Matrix_policy).dot(Reward_policy)\n",
        "    return value_function"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncqbPx99ncVY"
      },
      "source": [
        "def policy_iteration(P, R, gamma=0.9, tol=1e-20):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        P: np.array\n",
        "            transition matrix (NsxNaxNs)\n",
        "        R: np.array\n",
        "            reward matrix (NsxNa)\n",
        "        gamma: float\n",
        "            discount factor\n",
        "        tol: float\n",
        "            precision of the solution\n",
        "    Return:\n",
        "        policy: np.array\n",
        "            the final policy\n",
        "        V: np.array\n",
        "            the value function associated to the final policy\n",
        "    \"\"\"\n",
        "    Ns, Na = R.shape\n",
        "    V = np.zeros(Ns)\n",
        "    policy = np.zeros(Ns).astype(int)\n",
        "    it=0\n",
        "    while np.max(abs(policy_evaluation(P, R, policy,tol)-V))!=0 :\n",
        "\n",
        "      V=np.copy(policy_evaluation(P, R, policy,tol))\n",
        "      for i in range(Ns):\n",
        "        liste=[]\n",
        "        for a in range(Na):\n",
        "          n=0\n",
        "          for j in range(Ns):\n",
        "            n+=policy_evaluation(P, R, policy,tol)[j]*P[i,a,j]\n",
        "            #print(n)\n",
        "          liste=liste+[R[i,a]+n*gamma]\n",
        "        policy[i]=l.index(max(l))\n",
        "        it+=1\n",
        "      #print(\"new policy\", policy)\n",
        "      #print(\"new error\", np.max(abs(policy_evaluation(P, R, policy,tol)-V)))\n",
        "    return policy, V"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jB7dfA5nRCZ"
      },
      "source": [
        "def value_iteration(P, R, gamma=0.9, tol=1e-3):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        P: np.array\n",
        "            transition matrix (NsxNaxNs)\n",
        "        R: np.array\n",
        "            reward matrix (NsxNa)\n",
        "        gamma: float\n",
        "            discount factor\n",
        "        tol: float\n",
        "            precision of the solution\n",
        "    Return:\n",
        "        Q: final Q-function (at iteration n)\n",
        "        greedy_policy: greedy policy wrt Qn\n",
        "        Qfs: all Q-functions generated by the algorithm (for visualization)\n",
        "    \"\"\"\n",
        "    Q = np.zeros_like(R)\n",
        "    Ns, Na = R.shape\n",
        "\n",
        "    Qfs = []\n",
        "    B=np.copy(Q) + 1\n",
        "    while np.max(abs(Q-B))>tol:\n",
        "      B=np.copy(Q)\n",
        "      Qfs+=[B]\n",
        "\n",
        "      for i in range(Ns):\n",
        "        for a in range(Na):\n",
        "          s=0\n",
        "          for j in range(Ns):\n",
        "            s+=max(Q[j,])*P[i,a,j]\n",
        "          Q[i,a]=R[i,a]+s*gamma\n",
        "\n",
        "    greedy_policy = np.argmax(Q, axis=1)\n",
        "\n",
        "    return Q, greedy_policy, Qfs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Fi0IzZJp74Z"
      },
      "source": [
        "### Testing your code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7JKrc1oqFI2"
      },
      "source": [
        "# Parameters\n",
        "tol = 1e-5\n",
        "gamma = 0.99\n",
        "\n",
        "# Environment\n",
        "env = get_env()\n",
        "\n",
        "# run value iteration to obtain Q-values\n",
        "VI_Q, VI_greedypol, all_qfunctions =  value_iteration(env.P, env.R, gamma=gamma, tol=tol)\n",
        "# render the policy\n",
        "print(\"[VI]Greedy policy: \")\n",
        "render_policy(env, VI_greedypol)\n",
        "\n",
        "# compute the value function of the greedy policy using matrix inversion\n",
        "from numpy.linalg import inv\n",
        "Ns=31\n",
        "Matrix_policy=np.zeros((Ns,Ns))\n",
        "for i in range(Ns):\n",
        "  for j in range(Ns):\n",
        "    Matrix_policy[i,j]=env.P[i,VI_greedypol[i],j]\n",
        "Reward_policy=np.zeros((Ns,))\n",
        "for i in range(Ns):\n",
        "  Reward_policy[i]=env.R[i,VI_greedypol[i]]\n",
        "Value_greedy_policy=inv(np.identity(Ns)-gamma*Matrix_policy)@ Reward_policy\n",
        "\n",
        "\n",
        "# compute value function of the greedy policy\n",
        "#\n",
        "\n",
        "# ====================================================\n",
        "\n",
        "# show the error between the computed V-functions and the final V-function\n",
        "# (that should be the optimal one, if correctly implemented)\n",
        "# as a function of time\n",
        "final_V = all_qfunctions[-1].max(axis=1)\n",
        "norms = [ np.linalg.norm(q.max(axis=1) - final_V) for q in all_qfunctions]\n",
        "plt.plot(norms)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Error')\n",
        "plt.title(\"Value iteration: convergence\")\n",
        "\n",
        "#### POLICY ITERATION ####\n",
        "PI_policy, PI_V = policy_iteration(env.P, env.R, gamma=gamma, tol=tol)\n",
        "print(\"\\n[PI]final policy: \")\n",
        "render_policy(env, PI_policy)\n",
        "\n",
        "# Uncomment below to check that everything is correct\n",
        "assert np.allclose(PI_policy, VI_greedypol),\\\n",
        "    \"You should check the code, the greedy policy computed by VI is not equal to the solution of PI\"\n",
        "assert np.allclose(PI_V, greedy_V),\\\n",
        "    \"Since the policies are equal, even the value function should be\"\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2V1QdoH-xFX0"
      },
      "source": [
        "# Part 2 - Tabular RL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qf51VhoPxbV4"
      },
      "source": [
        "## Question 2.1\n",
        "\n",
        "The code below collects two datasets of transitions (containing states, actions, rewards and next states) for a discrete MDP.\n",
        "\n",
        "For each of the datasets:\n",
        "\n",
        "1. Estimate the transitions and rewards, $\\hat{P}$ and $\\hat{R}$.\n",
        "2. Compute the optimal value function and the optimal policy with respect to the estimated MDP (defined by $\\hat{P}$ and $\\hat{R}$), which we denote by $\\hat{\\pi}$ and $\\hat{V}$.\n",
        "3. Numerically compare the performance of $\\hat{\\pi}$ and $\\pi^\\star$ (the true optimal policy), and the error between $\\hat{V}$ and $V^*$ (the true optimal value function).\n",
        "\n",
        "Which of the two data collection methods do you think is better? Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWSyewG2EZpJ"
      },
      "source": [
        "### **Answer**\n",
        "\n",
        "The second data collection , because if the the random policy chosen to collect the first data collection is not good \" or optimal\", we will get very bad samples. On the other hand, it's less risky to collect samples for the the second data collection with a uniform policy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lNPhB28EcGd"
      },
      "source": [
        "def get_random_policy_dataset(env, n_samples):\n",
        "  \"\"\"Get a dataset following a random policy to collect data.\"\"\"\n",
        "  states = []\n",
        "  actions = []\n",
        "  rewards = []\n",
        "  next_states = []\n",
        "\n",
        "  state = env.reset()\n",
        "  for _ in range(n_samples):\n",
        "    action = env.action_space.sample()\n",
        "    next_state, reward, is_terminal, info = env.step(action)\n",
        "    states.append(state)\n",
        "    actions.append(action)\n",
        "    rewards.append(reward)\n",
        "    next_states.append(next_state)\n",
        "    # update state\n",
        "    state = next_state\n",
        "    if is_terminal:\n",
        "      state = env.reset()\n",
        "\n",
        "  dataset = (states, actions, rewards, next_states)\n",
        "  return dataset\n",
        "\n",
        "def get_uniform_dataset(env, n_samples):\n",
        "  \"\"\"Get a dataset by uniformly sampling states and actions.\"\"\"\n",
        "  states = []\n",
        "  actions = []\n",
        "  rewards = []\n",
        "  next_states = []\n",
        "  for _ in range(n_samples):\n",
        "    state = env.observation_space.sample()\n",
        "    action = env.action_space.sample()\n",
        "    next_state, reward, is_terminal, info = env.sample(state, action)\n",
        "    states.append(state)\n",
        "    actions.append(action)\n",
        "    rewards.append(reward)\n",
        "    next_states.append(next_state)\n",
        "\n",
        "  dataset = (states, actions, rewards, next_states)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "# Collect two different datasets\n",
        "num_samples = 500\n",
        "env = get_env()\n",
        "dataset_1 = get_random_policy_dataset(env, num_samples)\n",
        "dataset_2 = get_uniform_dataset(env, num_samples)\n",
        "\n",
        "\n",
        "# Item 3: Estimate the MDP with the two datasets; compare the optimal value\n",
        "# functions in the true and in the estimated MDPs\n",
        "\n",
        "#Dataset1:\n",
        "liste1=[]\n",
        "for i in range(len(dataset_1[0])):\n",
        "  liste1+=[(dataset_1[0][i],dataset_1[1][i],dataset_1[2][i],dataset_1[3][i])]\n",
        "  liste1_R=[(k[0],k[1]) for k in liste1]      #reward matrix\n",
        "\n",
        "#Reward matrix for the first dataset\n",
        "R1=np.zeros((env.Ns,env.Na))\n",
        "for i in range(len(dataset_1[0])):\n",
        "  R1[liste1[i][0],liste1[i][1]]+=liste1[i][2]\n",
        "for i in range(31):\n",
        "  for a in range(4):\n",
        "    if liste1_R.count((i,a))!=0:\n",
        "      R1[i,a]=R1[i,a]/liste1_R.count((i,a))\n",
        "    else:\n",
        "      R1[i,a]=0\n",
        "# Transisition matrix\n",
        "P1=np.zeros((env.Ns,env.Na, env.Ns))\n",
        "liste1_P=[(K[0],K[1],K[3]) for K in liste1]\n",
        "for i in liste1_P:\n",
        "  P1[i[0],i[1],i[2]]+=1/liste1_R.count((i[0],i[1]))\n",
        "\n",
        "# Dataset2:\n",
        "\n",
        "liste2=[]\n",
        "for i in range(len(dataset_2[0])):\n",
        "  liste2+=[(dataset_2[0][i],dataset_2[1][i],dataset_2[2][i],dataset_2[3][i])]\n",
        "liste2_R=[(k[0],k[1]) for k in liste2]\n",
        "R2=np.zeros((31,4))\n",
        "for i in range(len(dataset_2[0])):\n",
        "  R2[liste2[i][0],liste2[i][1]]+=liste2[i][2]\n",
        "for i in range(env.Ns):\n",
        "  for a in range(env.Na):\n",
        "    if liste2_R.count((i,a))!=0:\n",
        "      R2[i,a]=R2[i,a]/liste2_R.count((i,a))\n",
        "    else:\n",
        "      R2[i,a]=0\n",
        "\n",
        "\n",
        "#Transition matrix P2\n",
        "P2=np.zeros((env.Ns,env.Na,env.Ns))\n",
        "liste2_P=[(K[0],K[1],K[3]) for K in liste2]\n",
        "for i in liste2_P:\n",
        "  P2[i[0],i[1],i[2]]+=1/liste2_R.count((i[0],i[1]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QNmAfggUmaj"
      },
      "source": [
        "question : Optimal Policy\n",
        "\n",
        "Let's consider a different policy_iteration fuction, in a way that if speed= False, it will give the policy and the value function.\n",
        "\n",
        "if speed =True, it will return the policy, value function and the number of iterations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCsHaloGUlN2"
      },
      "source": [
        "def policy_iteration2(P, R, gamma=0.9, tol=1e-10,speed=False):#when it is True return also the number of iteration\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        P: np.array\n",
        "            transition matrix (NsxNaxNs)\n",
        "        R: np.array\n",
        "            reward matrix (NsxNa)\n",
        "        gamma: float\n",
        "            discount factor\n",
        "        tol: float\n",
        "            precision of the solution\n",
        "    Return:\n",
        "        policy: np.array\n",
        "            the final policy\n",
        "        V: np.array\n",
        "            the value function associated to the final policy\n",
        "    \"\"\"\n",
        "    Ns, Na = R.shape\n",
        "    V = np.zeros(Ns)\n",
        "    policy = np.ones(Ns, dtype=np.int)\n",
        "    it=0\n",
        "    while np.max(abs(policy_evaluation(P, R, policy,tol)-V))!=0 :\n",
        "      it+=1\n",
        "\n",
        "      V=np.copy(policy_evaluation(P, R, policy,tol))\n",
        "      for i in range(Ns):\n",
        "        l=[]\n",
        "        for a in range(Na):\n",
        "          n=0\n",
        "          for j in range(Ns):\n",
        "            n+=policy_evaluation(P, R, policy,tol)[j]*P[i,a,j]\n",
        "\n",
        "          l=l+[R[i,a]+n*gamma]\n",
        "        policy[i]=l.index(max(l))\n",
        "        it+=1\n",
        "\n",
        "    return  policy, V\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nrz3XZptWjrv"
      },
      "source": [
        "## MDP1\n",
        "optimal_policy1,value_function_1,nombre_d_iteration1  = policy_iteration2(P1, R2,speed=True)\n",
        "\n",
        "\n",
        "print(\"optimal_policy1\", optimal_policy1)\n",
        "print(\"value_function_1\", value_function_1)\n",
        "print(\"nombre_d_iteration1\", nombre_d_iteration1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7tlePjxWtgq"
      },
      "source": [
        "##MDP2\n",
        "optimal_policy2,value_function_2,nombre_d_iteration2 = policy_iteration2(P2, R2,speed=True)\n",
        "\n",
        "print(\"optimal_policy2\", optimal_policy2)\n",
        "print(\"value_function_2\", value_function_2)\n",
        "print(\"nombre_d_iteration2\", nombre_d_iteration2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5l0RvawXB-f"
      },
      "source": [
        "\n",
        "True_optimale_policy,V_optimale_policy,nombre_d_iteration = policy_iteration2(env.P,env.R,speed=True)\n",
        "\n",
        "\n",
        "True_optimale_policy,V_optimale_policy,nombre_d_iteration"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKINsa_yGLGL"
      },
      "source": [
        "## Question 2.2\n",
        "\n",
        "Suppose that $\\hat{P}$ and $\\hat{R}$ are estimated from a dataset of exactly $N$ i.i.d. samples from **each** state-action pair. This means that, for each $(s,a)$, we have $N$ samples $\\{(s_1',r_1, \\dots, s_N', r_N\\}$, where $s_i' \\sim P(\\cdot | s,a)$ and $r_i \\sim R(s,a)$ for $i=1,\\dots,N$, and\n",
        "$$ \\hat{P}(s'|s,a) = \\frac{1}{N}\\sum_{i=1}^N \\mathbb{1}(s_i' = s'), $$\n",
        "$$ \\hat{R}(s,a) = \\frac{1}{N}\\sum_{i=1}^N r_i.$$\n",
        "Suppose that $R$ is a distribution with support in $[0,1]$. Let $\\hat{V}$ be the optimal value function computed in the empirical MDP (i.e., the one with transitions $\\hat{P}$ and rewards $\\hat{R}$). For any $\\delta\\in(0,1)$, derive an upper bound to the error\n",
        "\n",
        "$$ \\| \\hat{V} - V^* \\|_\\infty $$\n",
        "\n",
        "which holds with probability at least $1-\\delta$.\n",
        "\n",
        "**Note** Your bound should only depend on deterministic quantities like $N$, $\\gamma$, $\\delta$, $S$, $A$. It should *not* dependent on the actual random samples.\n",
        "\n",
        "**Hint** The following two inequalities may be helpful.\n",
        "\n",
        "1. **A (simplified) simulation lemma**. For any state $\\bar{s}$,\n",
        "\n",
        "$$ |\\hat{V}(\\bar{s}) - V^*(\\bar{s})| \\leq \\frac{1}{1-\\gamma}\\max_{s,a} \\left| R(s,a) - \\hat{R}(s,a) + \\gamma \\sum_{s'}(P(s'|s,a) - \\hat{P}(s'|s,a)) V^*(s') \\right|$$\n",
        "\n",
        "2. **Hoeffding's inequality**. Let $X_1, \\dots X_N$ be $N$ i.i.d. random variables bounded in the interval $[0,b]$ for some $b>0$. Let $\\bar{X} = \\frac{1}{N}\\sum_{i=1}^N X_i$ be the empirical mean. Then, for any $\\epsilon > 0$,\n",
        "\n",
        "$$ \\mathbb{P}(|\\bar{X} - \\mathbb{E}[\\bar{X}]| > \\epsilon) \\leq 2e^{-\\frac{2N\\epsilon^2}{b^2}}.$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKmdulLaMoiN"
      },
      "source": [
        "### **Answer**\n",
        "\n",
        "\\begin{align*}\n",
        "\\mathbb{P}(\\|V^* - \\hat{V}\\|_\\infty > \\epsilon) &\\leq \\sum_{\\bar{s} \\in S} \\mathbb{P}(|V^*(\\bar{s})-\\hat{V}(\\bar{s})|> \\epsilon)\\\\\n",
        "&\\leq \\sum_{\\bar{s} \\in S} \\mathbb{P}\\left(\\frac{1}{1-\\gamma} \\max_{s,a} \\left| R(s,a) - \\hat{R}(s,a) + \\gamma \\sum_{s' \\in S} (P(s'|s,a) - \\hat{P}(s'|s,a)) V^*(s') \\right| > \\epsilon\\right) \\\\\n",
        "&\\leq \\sum_{\\bar{s} \\in S} \\mathbb{P}\\left(\\frac{1}{1-\\gamma} \\max_{s,a} \\left| R(s,a) + \\gamma \\sum_{s'\\in S} P(s'|s,a) V^*(s') - \\left(\\hat{R}(s,a) + \\gamma \\sum_{s'\\in S} \\hat{P}(s'|s,a) V^*(s')\\right)\\right| > \\epsilon\\right) \\\\\n",
        "&\\leq \\sum_{\\bar{s} \\in S} \\sum_{s,a} \\mathbb{P}\\left(\\frac{1}{1-\\gamma} \\left| Y - \\mathbb{E}[Y] \\right| > \\epsilon\\right),\n",
        "\\end{align*}\n",
        "\n",
        "where $ Y = \\hat{R}(s,a) + \\gamma \\sum_{s'\\in S} \\hat{P}(s'|s,a) V^*(s') = \\frac{1}{N} \\sum_{i=1}^N \\left(r_i + \\gamma \\sum_{s' \\in S} \\mathbb{1}_{\\{s_i'=s'\\}} V^*(s')\\right) = \\frac{1}{N} \\sum_{i=1}^N Y_i. $\n",
        "\n",
        "$ Y_i$ are i.i.d. random variables.\n",
        "\n",
        "- **Now let's show that $ Y_i \\in [0, 1 + \\gamma\\|V^*\\|_{\\infty}] $.**\n",
        "\n",
        "Since $ R $ is bounded, we assume its upper bound is 1. We have, for any policy $ \\pi $,\n",
        "\n",
        "$$V^{\\pi}(s) = \\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t) \\mid s_0 = s \\right] \\leq \\sum_{t=0}^{\\infty} \\gamma^t = \\frac{1}{1-\\gamma}.\n",
        "$$\n",
        "\n",
        "Therefore, $ Y_i \\in [0, \\frac{1}{1-\\gamma}] $. Applying Hoeffding's inequality to $ Y_i $:\n",
        "\n",
        "$$\n",
        "\\mathbb{P}(|Y - \\mathbb{E}[Y]| > \\epsilon) \\leq 2e^{-\\frac{2N\\epsilon^2}{b^2}},\n",
        "$$\n",
        "\n",
        "with $ N = N $, $ b = \\frac{1}{1-\\gamma} $.\n",
        "\n",
        "We get:\n",
        "\n",
        "$$\n",
        "\\mathbb{P}\\left(\\frac{1}{1-\\gamma} |Y - \\mathbb{E}[Y]| > \\epsilon\\right) = \\mathbb{P}\\left(|Y - \\mathbb{E}[Y]| > \\epsilon(1-\\gamma)\\right) \\leq 2e^{-2\\epsilon^2 N (1-\\gamma)^2}.\n",
        "$$\n",
        "\n",
        "Therefore,\n",
        "\n",
        "\\begin{align*}\n",
        "\\mathbb{P}(\\|V^* - \\hat{V}\\|_\\infty > \\epsilon) &\\leq \\sum_{s \\in S} \\sum_{s,a} 2e^{-2\\epsilon^2 N (1-\\gamma)^2} \\\\\n",
        "&\\leq 2 |A||S|^2 e^{-2\\epsilon^2 N (1-\\gamma)^2}.\n",
        "\\end{align*}\n",
        "\n",
        "Taking $ \\epsilon' = \\frac{1}{\\sqrt{2N}} \\times \\frac{1}{1-\\gamma} \\times \\sqrt{\\log\\left(\\frac{2|A||S|^2}{\\delta}\\right)} $,\n",
        "\n",
        "we finally get, for any $ \\delta \\in (0,1) $:\n",
        "\n",
        "$$\n",
        "\\mathbb{P}\\left(\\|V^* - \\hat{V}\\|_\\infty \\leq \\epsilon'\\right) \\geq 1 - \\delta.\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpqwCBG2MwxO"
      },
      "source": [
        "\n",
        "\n",
        "## Question 2.3\n",
        "\n",
        "Suppose once again that we are given a dataset of $N$ samples in the form of tuples $(s_i,a_i,s_i',r_i)$. We know that each tuple contains a valid transition from the true MDP, i.e., $s_i' \\sim P(\\cdot | s_i, a_i)$ and $r_i \\sim R(s_i,a_i)$, while the state-action pairs $(s_i,a_i)$ from which the transition started can be arbitrary.\n",
        "\n",
        "Suppose we want to apply Q-learning to this MDP. Can you think of a way to leverage this offline data to improve the sample-efficiency of the algorithm? What if we were using SARSA instead?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Answer**\n",
        "\n",
        "We can utilize this dataset by implementing a model-based approach like the Dyna-Q algorithm, which was introduced by Sutton and Barto. The Dyna-Q algorithm combines model-free and model-based learning by using the dataset to simulate additional experiences, thereby improving the sample efficiency.\n",
        "\n",
        "### **Q-learning with Dyna-Q:**\n",
        "\n",
        "1. **Direct Learning:** We first use the dataset to update the action-value function $Q$ directly using the standard Q-learning update rule:\n",
        "\n",
        "    $$\n",
        "    Q(s, a) \\leftarrow Q(s, a) + \\alpha_t(s, a) \\left(r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)\\right),\n",
        "    $$\n",
        "\n",
        "    where $(s, a, r, s')$ are sampled from the offline dataset.\n",
        "\n",
        "2. **Model Learning:** We can use the dataset to estimate the transition probabilities $\\hat{P}(s'|s, a)$ and rewards $\\hat{R}(s, a)$. Once the model is learned, we can simulate additional experiences by generating samples $(s', r)$ from the model given any state-action pair $(s, a)$.\n",
        "\n",
        "3. **Simulated Experience:** These simulated experiences can then be used to perform additional Q-learning updates, which enhances the sample efficiency by allowing the algorithm to learn from both real and simulated data.\n",
        "\n",
        "### **SARSA with Offline Data:**\n",
        "\n",
        "In the case of SARSA, which is an on-policy method, the algorithm updates the policy using transitions generated by the current policy. To leverage the offline data:\n",
        "\n",
        "1. **Initialization:** We initialize the action-value function $$Q(s, a)$ using the offline dataset. This can be done by iterating over the dataset and applying the SARSA update rule:\n",
        "\n",
        "    $$\n",
        "    Q(s, a) \\leftarrow Q(s, a) + \\alpha_t(s, a) \\left(r + \\gamma Q(s', a') - Q(s, a)\\right),\n",
        "    $$\n",
        "\n",
        "    where $(s, a, r, s')$ are sampled from the offline dataset, and $$a'$ is the action chosen according to the current policy $\\pi$.\n",
        "\n",
        "2. **Policy Update:** After updating $Q$, we update the policy $\\pi$ to be the $\\epsilon$-greedy policy derived from the updated $Q$ values.\n",
        "\n",
        "3. **Simulated Experience (Optional):** Similar to Dyna-Q, we can use the learned model to simulate additional experiences and further update the $Q$ values using the SARSA update rule.\n",
        "\n",
        "By leveraging the offline dataset in this way, both Q-learning and SARSA can improve their sample efficiency, leading to faster convergence to the optimal policy.\n"
      ],
      "metadata": {
        "id": "x6Yqfj7dUe0b"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "542QxKsSOs21"
      },
      "source": [
        "# Part 3 - RL with Function Approximation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiGZBiJ4PiIE"
      },
      "source": [
        "## Question 3.1\n",
        "\n",
        "Given a datset $(s_i, a_i, r_i, s_i')$ of (states, actions, rewards, next states), the Fitted Q-Iteration (FQI) algorithm proceeds as follows:\n",
        "\n",
        "\n",
        "* We start from a $Q$ function $Q_0 \\in \\mathcal{F}$, where $\\mathcal{F}$ is a function space;\n",
        "* At every iteration $k$, we compute $Q_{k+1}$ as:\n",
        "\n",
        "$$\n",
        "Q_{k+1}\\in\\arg\\min_{f\\in\\mathcal{F}} \\frac{1}{2}\\sum_{i=1}^N\n",
        "\\left(\n",
        "  f(s_i, a_i) - y_i^k\n",
        "\\right)^2 + \\lambda \\Omega(f)\n",
        "$$\n",
        "where $y_i^k = r_i + \\gamma \\max_{a'}Q_k(s_i', a')$, $\\Omega(f)$ is a regularization term and $\\lambda > 0$ is the regularization coefficient.\n",
        "\n",
        "\n",
        "Consider FQI with *linear* function approximation. That is, for a given feature map $\\phi : S \\rightarrow \\mathbb{R}^d$, we consider a parametric family of $Q$ functions $Q_\\theta(s,a) = \\phi(s)^T\\theta_a$ for $\\theta_a\\in\\mathbb{R}^d$. Suppose we are applying FQI on a given dataset of $N$ tuples of the form $(s_i, a_i, r_i, s_i')$ and we are at the $k$-th iteration. Let $\\theta_k \\in\\mathbb{R}^{d \\times A}$ be our current parameter. Derive the *closed-form* update to find $\\theta_{k+1}$, using $\\frac{1}{2}\\sum_a ||\\theta_a||_2^2$ as regularization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jx7aE41DkEM"
      },
      "source": [
        "### **Answer**\n",
        "\n",
        "We consider the following function:\n",
        "\n",
        "$$\n",
        "K(\\theta) = \\frac{1}{2} \\sum_{i=1}^N \\left(\\phi(s_i)^T \\theta_{a_i} - y_i^k\\right)^2 + \\lambda \\frac{1}{2} \\sum_a \\|\\theta_a\\|_2^2.\n",
        "$$\n",
        "\n",
        "The gradient of $K(\\theta)$ with respect to $\\theta_a$ is:\n",
        "\n",
        "$$\n",
        "\\nabla_{\\theta_a} K(\\theta) = \\sum_{i: a_i = a} \\left(\\phi(s_i)^T \\theta_a - y_i^k\\right)\\phi(s_i) + \\lambda \\theta_a = \\left(\\sum_{i: a_i = a} \\phi(s_i) \\phi(s_i)^T + \\lambda \\mathbb{I}_d\\right)\\theta_a - \\sum_{i: a_i = a} y_i^k \\phi(s_i).\n",
        "$$\n",
        "\n",
        "Setting $\\nabla_{\\theta_a} K(\\theta) = 0$ gives us:\n",
        "\n",
        "$$\n",
        "\\theta_a = \\left(\\sum_{i: a_i = a} \\phi(s_i) \\phi(s_i)^T + \\lambda \\mathbb{I}_d\\right)^{-1} \\left(\\sum_{i: a_i = a} y_i^k \\phi(s_i)\\right).\n",
        "$$\n",
        "\n",
        "Thus, the updated parameters at iteration $k+1$ are:\n",
        "\n",
        "$$\n",
        "\\theta_{k+1} = (\\theta_a)_{a \\in \\mathcal{A}}.\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewHzjm7MVGBg"
      },
      "source": [
        "\n",
        "## Question 3.2\n",
        "\n",
        "The code below creates a larger gridworld (with more states than the one used in the previous questions), and defines a feature map. Implement linear FQI to this environment (in the function `linear_fqi()` below), and compare the approximated $Q$ function to the optimal $Q$ function computed with value iteration.\n",
        "\n",
        "Can you improve the feature map in order to reduce the approximation error?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tu4g-HSnEcBs"
      },
      "source": [
        "### **Answer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZovF3VXOVfCs"
      },
      "source": [
        "def get_large_gridworld():\n",
        "  \"\"\"Creates an instance of a grid-world MDP with more states.\"\"\"\n",
        "  walls = [(ii, 10) for ii in range(15) if (ii != 7 and ii != 8)]\n",
        "  env = GridWorld(\n",
        "      nrows=15,\n",
        "      ncols=15,\n",
        "      reward_at = {(14, 14):1.0},\n",
        "      walls=tuple(walls),\n",
        "      success_probability=0.9,\n",
        "      terminal_states=((14, 14),)\n",
        "  )\n",
        "  return env\n",
        "\n",
        "\n",
        "class GridWorldFeatureMap:\n",
        "  \"\"\"Create features for state-action pairs\n",
        "\n",
        "  Args:\n",
        "    dim: int\n",
        "      Feature dimension\n",
        "    sigma: float\n",
        "      RBF kernel bandwidth\n",
        "  \"\"\"\n",
        "  def __init__(self, env, dim=15, sigma=0.25):\n",
        "    self.index2coord = env.index2coord\n",
        "    self.n_states = env.Ns\n",
        "    self.n_actions = env.Na\n",
        "    self.dim = dim\n",
        "    self.sigma = sigma\n",
        "\n",
        "    n_rows = env.nrows\n",
        "    n_cols = env.ncols\n",
        "\n",
        "    # build similarity matrix\n",
        "    sim_matrix = np.zeros((self.n_states, self.n_states))\n",
        "    for ii in range(self.n_states):\n",
        "        row_ii, col_ii = self.index2coord[ii]\n",
        "        x_ii = row_ii / n_rows\n",
        "        y_ii = col_ii / n_cols\n",
        "        for jj in range(self.n_states):\n",
        "            row_jj, col_jj = self.index2coord[jj]\n",
        "            x_jj = row_jj / n_rows\n",
        "            y_jj = col_jj / n_cols\n",
        "            dist = np.sqrt((x_jj - x_ii) ** 2.0 + (y_jj - y_ii) ** 2.0)\n",
        "            sim_matrix[ii, jj] = np.exp(-(dist / sigma) ** 2.0)\n",
        "\n",
        "    # factorize similarity matrix to obtain features\n",
        "    uu, ss, vh = np.linalg.svd(sim_matrix, hermitian=True)\n",
        "    self.feats = vh[:dim, :]\n",
        "\n",
        "  def map(self, observation):\n",
        "    feat = self.feats[:, observation].copy()\n",
        "    return feat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InCfu7F9-TbS"
      },
      "source": [
        "env = get_large_gridworld()\n",
        "feat_map = GridWorldFeatureMap(env)\n",
        "\n",
        "# Visualize large gridworld\n",
        "render_policy(env)\n",
        "\n",
        "# The features have dimension (feature_dim).\n",
        "feature_example = feat_map.map(1) # feature representation of s=1\n",
        "print(feature_example)\n",
        "\n",
        "# Initial vector theta representing the Q function\n",
        "theta = np.zeros((feat_map.dim, env.action_space.n))\n",
        "print(theta.shape)\n",
        "print(feature_example @ theta) # approximation of Q(s=1, a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p21KMmruugO1"
      },
      "source": [
        "def linear_fqi(env, feat_map, num_iterations, lambd=0.1, gamma=0.95):\n",
        "  \"\"\"\n",
        "  # Linear FQI implementation\n",
        "  # TO BE COMPLETED\n",
        "  \"\"\"\n",
        "  n_samples=500\n",
        "  # get a dataset\n",
        "  dataset = get_uniform_dataset(env, n_samples=n_samples)\n",
        "\n",
        "  # OR dataset = get_random_policy_dataset(env, n_samples=...)\n",
        "\n",
        "\n",
        "  theta = np.zeros((feat_map.dim, env.Na))\n",
        "  Q= np.zeros((env.Ns, env.Na))\n",
        "\n",
        "  for it in range(num_iterations):\n",
        "\n",
        "    for a in range(env.Na):\n",
        "      u=0\n",
        "      v=0\n",
        "      index = np.where(np.array(dataset[1]) == 0)[0]\n",
        "      for i in index:\n",
        "        u+= np.outer(feat_map.map(dataset[0][i]), (feat_map.map(dataset[0][i])))\n",
        "        v+= feat_map.map(dataset[0][i])* y_i_\n",
        "\n",
        "      z=z=inv(u+lambd*np.eye(feat_map.dim)) @ v\n",
        "      theta[:,a]=z\n",
        "      for s in range(env.Ns):\n",
        "        Q[s,a]= np.vdot(feat_map.map(s), theta[:, a])\n",
        "    pass\n",
        "\n",
        "  return theta\n",
        "\n",
        "# ----------------------------\n",
        "# Environment and feature map\n",
        "# ----------------------------\n",
        "env = get_large_gridworld()\n",
        "# you can change the parameters of the feature map, and even try other maps!\n",
        "feat_map = GridWorldFeatureMap(env, dim=15, sigma=0.25)\n",
        "\n",
        "theta = linear_fqi(env, feat_map, num_iterations=1000)\n",
        "\n",
        "# Compute and run greedy policy\n",
        "Q_fqi = np.zeros((env.Ns, env.Na))\n",
        "for ss in range(env.Ns):\n",
        "  state_feat = feat_map.map(ss)\n",
        "  Q_fqi[ss, :] = state_feat @ theta\n",
        "\n",
        "V_fqi = Q_fqi.max(axis=1)\n",
        "policy_fqi = Q_fqi.argmax(axis=1)\n",
        "render_policy(env, policy_fqi, horizon=100)\n",
        "\n",
        "# Visualize the approximate value function in the gridworld.\n",
        "img = env.get_layout_img(V_fqi)\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}