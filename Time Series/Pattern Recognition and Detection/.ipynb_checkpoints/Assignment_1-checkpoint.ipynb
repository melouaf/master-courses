{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afb72815-74ef-4b27-b413-26c1c4f20652",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77573173-e7b9-46b4-ae78-a0f9b0d87351",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263236fe-6d00-45aa-80f1-e36765758e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import argrelmax\n",
    "import matplotlib.pyplot as plt\n",
    "from loadmydata.load_human_locomotion import (\n",
    "    load_human_locomotion_dataset,\n",
    "    get_code_list,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b210a7-540e-40ae-81c5-790aec715b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD_IoU = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ce9a17-6c03-4343-b2d4-c5dc94eac178",
   "metadata": {},
   "source": [
    "**Utility functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba29ee2b-e6e5-4c50-8e2f-d522c420ee6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_step_list(step_list):\n",
    "    \"\"\"Some sanity checks.\"\"\"\n",
    "    for step in step_list:\n",
    "        assert len(step) == 2, f\"A step consists of a start and an end: {step}.\"\n",
    "        start, end = step\n",
    "        assert start < end, f\"start should be before end: {step}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ebd042-d807-4fa1-aa6b-d02b8836fcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inter_over_union(interval_1, interval_2):\n",
    "    \"\"\"Intersection over union for two intervals.\"\"\"\n",
    "    a, b = interval_1\n",
    "    c, d = interval_2\n",
    "    intersection = max(0, min(b, d) - max(a, c))\n",
    "    if intersection > 0:\n",
    "        union = max(b, d) - min(a, c)\n",
    "    else:\n",
    "        union = (b - a) + (d - c)\n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4743b9ee-2515-4d12-8d46-2b51d634b8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _step_detection_precision(step_list_true, step_list_pred):\n",
    "    \"\"\"Precision is the number of correctly predicted steps divided by the number of predicted\n",
    "    steps. A predicted step is counted as correct if it overlaps an annotated step (measured by the\n",
    "    \"intersection over union\" metric) by more than 75%.\n",
    "    Note that an annotated step can only be detected once. If several predicted steps correspond\n",
    "    to the same annotated step, all but one are considered as false.\n",
    "    Here, precision is computed on a single prediction task (all steps correspond to the same\n",
    "    signal).\n",
    "    The lists y_true_ and y_pred are lists of steps, for instance:\n",
    "        - step_list_true: [[357, 431], [502, 569], [633, 715], [778, 849], [907, 989]]\n",
    "        - step_list_pred: [[293, 365], [422, 508], [565, 642], [701, 789]]\n",
    "    Arguments:\n",
    "        step_list_true {List} -- list of true steps\n",
    "        step_list_pred {List} -- list of predicted steps\n",
    "    Returns:\n",
    "        float -- precision, between 0.0 and 1.0\n",
    "    \"\"\"\n",
    "    _check_step_list(step_list_pred)\n",
    "\n",
    "    if len(step_list_pred) == 0:  # empty prediction\n",
    "        return 0.0\n",
    "\n",
    "    n_correctly_predicted = 0\n",
    "    detected_index_set = set()  # set of index of detected true steps\n",
    "    for step_pred in step_list_pred:\n",
    "        for (index, step_true) in enumerate(step_list_true):\n",
    "            if (index not in detected_index_set) and (\n",
    "                inter_over_union(step_pred, step_true) > THRESHOLD_IoU\n",
    "            ):\n",
    "                n_correctly_predicted += 1\n",
    "                detected_index_set.add(index)\n",
    "                break\n",
    "    return n_correctly_predicted / len(step_list_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027b8b1c-4dfe-445f-829b-191af50a4c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _step_detection_recall(step_list_true, step_list_pred):\n",
    "    \"\"\"Recall is the number of detected annotated steps divided by the total number of annotated\n",
    "    steps. An annotated step is counted as detected if it overlaps a predicted step (measured by\n",
    "    the \"intersection over union\" metric) by more than 75%.\n",
    "    Note that an annotated step can only be detected once. If several annotated steps are detected\n",
    "    with the same predicted step, all but one are considered undetected.\n",
    "    Here, recall is computed on a single prediction task (all steps correspond to the same\n",
    "    signal).\n",
    "    The lists y_true_ and y_pred are lists of steps, for instance:\n",
    "        - step_list_true: [[357, 431], [502, 569], [633, 715], [778, 849], [907, 989]]\n",
    "        - step_list_pred: [[293, 365], [422, 508], [565, 642], [701, 789]]\n",
    "    Arguments:\n",
    "        step_list_true {List} -- list of true steps\n",
    "        step_list_pred {List} -- list of predicted steps\n",
    "    Returns:\n",
    "        float -- recall, between 0.0 and 1.0\n",
    "    \"\"\"\n",
    "    _check_step_list(step_list_pred)\n",
    "\n",
    "    n_detected_true = 0\n",
    "    predicted_index_set = set()  # set of indexes of predicted steps\n",
    "\n",
    "    for step_true in step_list_true:\n",
    "        for (index, step_pred) in enumerate(step_list_pred):\n",
    "            if (index not in predicted_index_set) and (\n",
    "                inter_over_union(step_pred, step_true) > THRESHOLD_IoU\n",
    "            ):\n",
    "                n_detected_true += 1\n",
    "                predicted_index_set.add(index)\n",
    "                break\n",
    "    return n_detected_true / len(step_list_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6020c8-03fb-4271-b94c-f6567a5654e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_step_detection(y_true, y_pred) -> float:\n",
    "    \"\"\"\n",
    "    Calculate f-score (geometric mean between precision and recall) for each instance (each\n",
    "    signal) and return the weighted average over instances.\n",
    "    The lists y_true_ and y_pred are lists of lists of steps, for instance:\n",
    "        - y_true: [[[907, 989]] [[357, 431], [502, 569]], [[633, 715], [778, 849]]]\n",
    "        - y_pred: [[[293, 365]], [[422, 508], [565, 642]], [[701, 789]]]\n",
    "    Arguments:\n",
    "        y_true {List} -- true steps\n",
    "        y_pred {List} -- predicted steps\n",
    "    Returns:\n",
    "        float -- f-score, between 0.0 and 1.0\n",
    "    \"\"\"\n",
    "    # to prevent throwing an exception when passing empty lists\n",
    "    if len(y_true) == 0:\n",
    "        return 0\n",
    "\n",
    "    fscore_list = list()\n",
    "\n",
    "    for (step_list_true, step_list_pred) in zip(y_true, y_pred):\n",
    "        prec = _step_detection_precision(step_list_true, step_list_pred)\n",
    "        rec = _step_detection_recall(step_list_true, step_list_pred)\n",
    "        if prec + rec < 1e-6:\n",
    "            fscore_list.append(0.0)\n",
    "        else:\n",
    "            fscore_list.append((2 * prec * rec) / (prec + rec))\n",
    "\n",
    "    return np.mean(fscore_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55792e82-63ac-494c-9837-ace94196ff73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparsify_codes(z_1D, atom_length: int):\n",
    "    \"\"\"Set to zero codes that are too close to each others.\n",
    "    \n",
    "    Keep only the (local) maximum code and set to zero codes that are less than\n",
    "    `atom_length` away.\n",
    "    `z_1D` is assumed to be univariate.\n",
    "    \"\"\"\n",
    "    res = np.zeros_like(z_1D)\n",
    "    argmax_indexes, = argrelmax(z_1D, order=atom_length)\n",
    "    res[argmax_indexes] = z_1D[argmax_indexes]\n",
    "    return res\n",
    "\n",
    "\n",
    "def sparse_codes_to_list_of_steps(z_1D, atom_length: int):\n",
    "    \"\"\"Return a list of steps from a 1D activation vector.\"\"\"\n",
    "    z_1D_sparser = sparsify_codes(z_1D=z_1D, atom_length=atom_length)\n",
    "    start_array, = np.nonzero(z_1D_sparser)\n",
    "    end_array = start_array + atom_length\n",
    "    return np.c_[start_array, end_array].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53469a53-92a0-403d-9bb4-544cd0e7a1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig_ax(figsize=(15, 5)):\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.autoscale(enable=True, axis='x', tight=True)\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacbbbed-3869-40b6-a757-1db94a608a83",
   "metadata": {},
   "source": [
    "# Convolutional dictionary learning (CDL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff83bc18-cf67-4397-b9dd-623804823938",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data\n",
    "\n",
    "This data set consists of signals collected with inertial measurement units (accelerometer+gyroscope), from 230 subjects undergoing a fixed protocol:\n",
    "- standing still,\n",
    "- walking 10 m,\n",
    "- turning around,\n",
    "- walking back,\n",
    "- stopping.\n",
    "\n",
    "In this assignment, we only consider the vertical acceleration of the left foot and all signals are truncated to 20 seconds (as a result, they all have same length). Signals are sampled at 100 Hz.\n",
    "\n",
    "The measured population is composed of healthy subjects as well as patients with neurological or orthopedic disorders.\n",
    "\n",
    "The start and end time stamps of thousands of footsteps are available.\n",
    "\n",
    "The data are part of a larger data set described in [1].\n",
    "\n",
    "[1] Truong, C., Barrois-MÃ¼ller, R., Moreau, T., Provost, C., Vienne-Jumeau, A., Moreau, A., Vidal, P.-P., Vayatis, N., Buffat, S., Yelnik, A., Ricard, D., & Oudre, L. (2019). A data set for the study of human locomotion with inertial measurements units. Image Processing On Line (IPOL), 9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984c4eac-ca2e-4810-801c-01f0bf51e51d",
   "metadata": {
    "tags": []
   },
   "source": [
    "The following cell defines the training set `(X_train, y_train)` and testing set `(X_test, y_test)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f749bf-a3f2-463c-81ed-d7169e754d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_indexes_train = [851, 428, 739, 621, 147, 281, 95, 619, 441, 149, 951, 803, 214, 754, 34, 516, 684, 514, 465, 675, 654, 665, 297, 217, 618, 37, 954, 888, 630, 839, 897, 146, 559, 896, 941, 93, 658, 674, 78, 498, 575, 525, 36, 313, 300, 710, 56, 460, 397, 943]\n",
    "subset_indexes_test = [683, 259, 59, 387, 634, 611, 87, 201, 86, 849, 538, 962, 205, 15, 883, 42]\n",
    "\n",
    "code_list = get_code_list()\n",
    "\n",
    "X_train = list()  # list of signals\n",
    "y_train = list()  # list of list of steps (the \"labels\")\n",
    "list_of_pathologies_train = list()\n",
    "\n",
    "X_test = list()  # list of signals\n",
    "y_test = list()  # list of list of steps (the \"labels\")\n",
    "list_of_pathologies_test = list()\n",
    "\n",
    "\n",
    "for (X, y, list_of_pathologies, subset_indexes) in zip([X_train, X_test], [y_train, y_test], [list_of_pathologies_train, list_of_pathologies_test], [subset_indexes_train, subset_indexes_test]):\n",
    "    for code in np.take(code_list, subset_indexes):\n",
    "        single_trial = load_human_locomotion_dataset(code)\n",
    "        signal = single_trial.signal.LAZ.to_numpy()  # keeping only one dimension (from the left sensor)\n",
    "        steps = single_trial.left_steps\n",
    "        X.append(signal[:2000])  # truncate signals to have the same length \n",
    "        y.append(steps[(steps<2000).prod(axis=1).astype(bool)])\n",
    "        list_of_pathologies.append(single_trial.metadata[\"PathologyGroup\"])\n",
    "    \n",
    "X_train = np.asarray(X_train)\n",
    "X_test = np.asarray(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927d5a1e-c719-4c5b-8875-e31f056dfadd",
   "metadata": {},
   "source": [
    "Display one signal. Notice the repetitive patterns: those are the footsteps to detect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c292b71-cf4e-424b-94e3-7ee4520972c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 45  # choose a signal\n",
    "(signal, steps, pathology) = X_train[ind], y_train[ind], list_of_pathologies_train[ind]\n",
    "\n",
    "# plotting and saving the figure\n",
    "fig, ax = fig_ax()\n",
    "ax.plot(signal)\n",
    "_ = ax.set_title(f\"Pathology group: {pathology}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd93f734-c888-44e7-beb1-54de8c3f19f2",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f5acdf-7c76-4271-9117-130e8d68482a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphacsc import learn_d_z\n",
    "from alphacsc.utils import construct_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67398caa-6f7c-4e87-bedd-ad95476a12c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 45  # choose a signal\n",
    "(signal, steps, pathology) = X_train[ind], y_train[ind], list_of_pathologies_train[ind]\n",
    "\n",
    "### Change this part (at least)\n",
    "reconstruction = np.zeros(signal.shape)  # add your own reconstruction\n",
    "mse = 0\n",
    "###\n",
    "\n",
    "# plotting and saving the figure\n",
    "fig, ax = fig_ax()\n",
    "ax.set_title(f\"MSE: {mse:.2f}\")\n",
    "ax.plot(signal, label=\"Original\")\n",
    "ax.plot(reconstruction, label=\"Reconstruction\")\n",
    "plt.legend()\n",
    "# saving the figure\n",
    "plt.savefig(fname=\"figure-question-3.pdf\", dpi=200, transparent=True, bbox_inches=\"tight\", pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e20d19-60b2-445f-9af3-abf5705ae6b8",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "Hints:\n",
    "- For ease of use, code a scikit-learn estimator class `ConvDL` that implements \n",
    "    - `.fit()` (learn dictionary),\n",
    "    - `.predict()` method (return a list of list of steps). Use the helper function `sparse_codes_to_list_of_steps`.\n",
    "- In the cross-validation, use the `f1_score_step_detection` function to compute the F-score between prediction and label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8a21e9-0d49-4d20-8235-48affec2236c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphacsc import update_z, learn_d_z\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class ConvDL(BaseEstimator):\n",
    "    def __init__(self, reg, n_atoms, atom_length, n_iter=30):\n",
    "        ...\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        ...\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6f8dd2-dbc8-4274-82cf-05f1f389b40b",
   "metadata": {},
   "source": [
    "Use helper functions from scikit-learn to find the optimal combination of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3be519-de16-446a-a474-4e1b91b96ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# list of values for lambda as a percentage of lambda_max (see Question 2)\n",
    "# use the argument `lmbd_max=\"scaled\"` in alphacsc.learn_d_z \n",
    "penalty_list = [0.1, 0.2, 0.5, 0.8, 0.9, 0.95]\n",
    "\n",
    "# list of values for K\n",
    "n_atoms_list = [2, 4, 6, 8, 10, 20]\n",
    "\n",
    "# list of values for L\n",
    "atom_length_list = [20, 50, 80, 100, 150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3db46d9-60c5-4a1e-9705-d9cb16319646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use X_train and y_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adadbf6-22d6-4854-ae1b-ae19b1b463c5",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0214a0c7-7bd6-4ca6-b46f-d268b3b4232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapt this code to your variables\n",
    "\n",
    "dictionary = np.zeros((3, 100))  # add the learned dictionary\n",
    "\n",
    "for (k, atom) in enumerate(dictionary):\n",
    "    fig, ax = fig_ax(figsize=(5, 3))\n",
    "    ax.plot(atom)\n",
    "    # saving the figure\n",
    "    plt.savefig(fname=f\"figure-question-5-atom-{k}.pdf\", dpi=200, transparent=True, bbox_inches=\"tight\", pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08f7e22-fe7b-4d14-9ef6-9722f898251f",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5892c5fa-a1f2-4a1f-8462-2eea29b0f457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use X_test and y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf9a94c-244a-470b-b9d6-ce9be480abfa",
   "metadata": {},
   "source": [
    "# Dynamic time warping (DTW)\n",
    "\n",
    "For this section, the data remain the same but the task is different. We want to classify footsteps in healthy/non-healthy (instead of detecting them as before)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06881b74-97b2-48ea-8685-ee830d6d3841",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0255e82c-dd54-4e9e-bea8-50284f2588d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_indexes = [95, 619, 441, 149, 951, 803, 214, 34, 37, 630]\n",
    "code_list = get_code_list()\n",
    "\n",
    "X_train = list()  # list of footstep signals\n",
    "y_train = list()  # list of pathologies (the \"labels\")\n",
    "\n",
    "for code in np.take(code_list, subset_indexes):\n",
    "    single_trial = load_human_locomotion_dataset(code)\n",
    "    signal = single_trial.signal.LAZ.to_numpy()  # keeping only one dimension (from the left sensor)\n",
    "    steps = single_trial.left_steps\n",
    "    pathology = single_trial.metadata[\"PathologyGroup\"]\n",
    "    label = 0 if pathology==\"Healthy\" else 1  # 0: healthy, 1: non-healthy\n",
    "    for (start, end) in steps:\n",
    "        X_train.append(signal[start:end])\n",
    "        y_train.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3c6563-fb1a-466a-9ee5-5b91dd1317fe",
   "metadata": {},
   "source": [
    "fig, ax = fig_ax()\n",
    "for (step_signal, label) in zip(X_train, y_train):\n",
    "    color = \"b\" if label==0 else \"r\"\n",
    "    ax.plot(step_signal, color=color, alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afca739f-9c05-432b-8732-5d8370299983",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf83669-5bc2-4103-9f83-b1c763ffcc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from dtw import dtw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012f8ef6-37f6-43ff-9e9e-63c33ca46aee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f0f89c5-e44a-40bb-96ae-0af1a8d735ac",
   "metadata": {},
   "source": [
    "## Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d12a4c-697b-4465-98ed-900f279e864c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
